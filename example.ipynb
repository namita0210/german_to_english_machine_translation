{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import dump\n",
    "from unicodedata import normalize\n",
    "from numpy import array\n",
    "import re\n",
    "import string\n",
    "from pickle import load\n",
    "from pickle import dump\n",
    "from numpy.random import rand\n",
    "from numpy.random import shuffle\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(filename):\n",
    "    file = open(filename,mode='rt')\n",
    "    text=file.read()\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi  Hallo\\nGood morning!   Guten Morgen\\nGood day    Guten Tag\\nI am a student  Ich bin studentin\\nI am twenty three years old Ich bin drie und swanzig '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_file('example.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split loaded doc into sentences\n",
    "def into_sentences(doc):\n",
    "    lines = doc.strip().split('\\n')\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi  Hallo',\n",
       " 'Good morning!   Guten Morgen',\n",
       " 'Good day    Guten Tag',\n",
       " 'I am a student  Ich bin studentin',\n",
       " 'I am twenty three years old Ich bin drie und swanzig']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "into_sentences(load_file('example.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split loaded doc by tabs\n",
    "def into_tab(doc):\n",
    "    lines = doc.strip().split('\\n')\n",
    "    pairs = [line.split('\\t') for line in lines]\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['hi  Hallo'],\n",
       " ['Good morning!   Guten Morgen'],\n",
       " ['Good day    Guten Tag'],\n",
       " ['I am a student  Ich bin studentin'],\n",
       " ['I am twenty three years old Ich bin drie und swanzig']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tab=into_tab(load_file('example.txt'))\n",
    "tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(lines):\n",
    "    cleaned = []\n",
    "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "    table = str.maketrans('','', string.punctuation)\n",
    "\n",
    "    for l in lines:\n",
    "        clean_pair=[]\n",
    "        for x in l :\n",
    "            x = normalize('NFD',x).encode('ascii','ignore')\n",
    "            x = x.decode('UTF-8')\n",
    "            x = x.split()\n",
    "            x = [word.lower() for word in x]\n",
    "            x = [word.translate(table) for word in x]\n",
    "            x = [re_print.sub('' , word) for word in x]\n",
    "            x = [word for word in x if word.isalpha()]\n",
    "            clean_pair.append(' '.join(x))\n",
    "        cleaned.append(clean_pair)\n",
    "    return array(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['hi hallo'],\n",
       "       ['good morning guten morgen'],\n",
       "       ['good day guten tag'],\n",
       "       ['i am a student ich bin studentin'],\n",
       "       ['i am twenty three years old ich bin drie und swanzig']],\n",
       "      dtype='<U52')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean(tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "string.printable\n",
    "a = 'hehe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice in wonderland is an amazing children's book\n"
     ]
    }
   ],
   "source": [
    "name = \"Alice\"\n",
    "statement = \"%s in wonderland is an amazing children's book\" %name\n",
    "print(statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['email@example.com', 'support@company.co.uk', 'namitabagri@gmail.com']\n"
     ]
    }
   ],
   "source": [
    "email_pattern = re.compile(r'\\b[\\w.-]+@[a-zA-Z-]+\\.[a-zA-Z.]{2,6}\\b')\n",
    "text = \"Contact us at email@example.com or support@company.co.uk for assistance. Also call namitabagri@gmail.com\"\n",
    "email_matches = email_pattern.findall(text)\n",
    "print(email_matches)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: Escaping characters like ^ and $ in regex.\n",
      "Escaped text: Escaping\\ characters\\ like\\ \\^\\ and\\ \\$\\ in\\ regex\\.\n"
     ]
    }
   ],
   "source": [
    "text_with_special_chars = \"Escaping characters like ^ and $ in regex.\"\n",
    "escaped_text = re.escape(text_with_special_chars)\n",
    "\n",
    "print(\"Original text:\", text_with_special_chars)\n",
    "print(\"Escaped text:\", escaped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re.compile('[^0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"\\\\#\\\\$%\\\\&\\'\\\\(\\\\)\\\\*\\\\+,\\\\-\\\\./:;<=>\\\\?@\\\\[\\\\\\\\\\\\]\\\\^_`\\\\{\\\\|\\\\}\\\\~\\\\ \\\\\\t\\\\\\n\\\\\\r\\\\\\x0b\\\\\\x0c]')\n"
     ]
    }
   ],
   "source": [
    "re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "print(re_print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "    Original :  This is just a demo, My name is Namita! What is your name?\n",
      "    No Punctuation :  This is just a demo My name is Namita What is your name\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demo_sentence_to_Remove_punctuation = \" This is just a demo, My name is Namita! What is your name?\"\n",
    "table = str.maketrans('','', string.punctuation)\n",
    "no_punctuation=demo_sentence_to_Remove_punctuation.translate(table)\n",
    "print(f''' \n",
    "    Original : {demo_sentence_to_Remove_punctuation}\n",
    "    No Punctuation : {no_punctuation}\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(lines):\n",
    "    cleaned = []\n",
    "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "    table = str.maketrans('','', string.punctuation)\n",
    "\n",
    "    for l in lines:\n",
    "        clean_pair=[]\n",
    "        for x in l :\n",
    "            x = normalize('NFD',x).encode('ascii','ignore')\n",
    "            x = x.decode('UTF-8')\n",
    "            x = x.split()\n",
    "            x = [word.lower() for word in x]\n",
    "            x = [word.translate(table) for word in x]\n",
    "            x = [re_print.sub('' , word) for word in x]\n",
    "            x = [word for word in x if word.isalpha()]\n",
    "            clean_pair.append(' '.join(x))\n",
    "        cleaned.append(clean_pair)\n",
    "    return array(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_clean_data(sentences , filename):\n",
    "    dump(sentences , open(filename, 'wb'))\n",
    "    print('Saved %s'%filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working code together - till this point - Cleaning and saving pkl file\n",
    "# Step-1 : Load the data\n",
    "def load_data(filename):\n",
    "    file = open(filename , 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# Step-2 : Spilt the data by lines and by tab\n",
    "def split_data(txt):\n",
    "    lines = txt.strip().split('\\n')\n",
    "    phrase = [ l.split('\\t') for l in lines]\n",
    "    return phrase\n",
    "\n",
    "# Step - 3 : Clean the phrases\n",
    "def clean_split_data(lines):\n",
    "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "    table = str.maketrans('','',string.punctuation)\n",
    "\n",
    "    cleaned =[]\n",
    "    for l in lines:\n",
    "        clean_pair=[]\n",
    "        for x in l:\n",
    "            x = normalize('NFD',x).encode('ascii','ignore')\n",
    "            x = x.decode('UTF-8')\n",
    "            x = x.split()\n",
    "            x = [word.lower() for word in x]\n",
    "            x = [word.translate(table) for word in x]\n",
    "            x = [re_print.sub('' , word) for word in x]\n",
    "            x = [word for word in x if word.isalpha()]\n",
    "            clean_pair.append(' '.join(x))\n",
    "        cleaned.append(clean_pair)\n",
    "    return array(cleaned)\n",
    "\n",
    "# Step - 4 : Save the cleaned data with pkl API\n",
    "def save_clean_data(sentences , filename):\n",
    "    dump(sentences , open(filename , 'wb'))\n",
    "    print('Saved %s'%filename)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved english-german.pkl\n"
     ]
    }
   ],
   "source": [
    "#Execution - part1\n",
    "text = load_data('example.txt')\n",
    "split_data(text)\n",
    "clean_split_data(split_data(text))\n",
    "save_clean_data(clean_split_data(split_data(text)) , 'english-german.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting\n",
    "# Step-5 : Load the pkl file\n",
    "def load_clean_sentences(filename):\n",
    "    return load(open(filename , 'rb'))\n",
    "\n",
    "# Step-6 : Save train-test to respective pkl files\n",
    "def save_clean_data(dataset,filename):\n",
    "    dump(dataset , open(filename , 'wb'))\n",
    "    print('Saved %s'%filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execution - part2\n",
    "raw_dataset=load_clean_sentences('english-german.pkl')\n",
    "\n",
    "# Step 6.1 : Reduce dataset size\n",
    "n_sentences = 10000\n",
    "dataset = raw_dataset[:n_sentences , :]   \n",
    "shuffle(dataset)\n",
    "\n",
    "#Step 6.2 : Split into train-test\n",
    "train, test = dataset[:9000], dataset[9000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved english-german-both.pkl\n",
      "Saved english-german-train.pkl\n",
      "Saved english-german-test.pkl\n"
     ]
    }
   ],
   "source": [
    "#Execution - part2.1\n",
    "save_clean_data(dataset , 'english-german-both.pkl')\n",
    "save_clean_data(train , 'english-german-train.pkl')\n",
    "save_clean_data(test , 'english-german-test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Neural Translation model\n",
    "#Start by loading the train - test data stored in pkl files\n",
    "# Step 7 : Load the pkl files\n",
    "\n",
    "dataset = load_clean_sentences('english-german-both.pkl')\n",
    "train = load_clean_sentences('english-german-train.pkl')\n",
    "test = load_clean_sentences('english-german-test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can use the Keras Tokenize class to map words to integers, as needed for modeling. \n",
    "#We will use separate tokenizer for the English sequences and the German sequences. \n",
    "#The function below-named create_tokenizer() will train a tokenizer on a list of phrases\n",
    "#Step 8 : Create tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
