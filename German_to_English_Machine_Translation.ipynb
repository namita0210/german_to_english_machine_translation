{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/namita0210/german_to_english_machine_translation/blob/main/German_to_English_Machine_Translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "NHdFZ05SX0RQ"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "from numpy import array\n",
        "from pickle import dump\n",
        "from unicodedata import normalize\n",
        "from pickle import load\n",
        "from pickle import dump\n",
        "from numpy.random import rand\n",
        "from numpy.random import shuffle\n",
        "from pickle import load\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.callbacks import ModelCheckpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SpUqCSOiJq19",
        "outputId": "f95d5450-87c2-49e4-f776-9f730a26d77c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "data = '/content/drive/MyDrive/deu.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "zs2n5AAzKyd5"
      },
      "outputs": [],
      "source": [
        "#function to load the file and preserve the unicode german characters\n",
        "def load_file(filename):\n",
        "  file = open(filename , 'r', encoding='utf-8')\n",
        "  text = file.read()\n",
        "  file.close()\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dit3yCEmLQB3",
        "outputId": "eddbd5d6-f164-4097-8633-46fb37e5b137"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hi.\tHallo!\n",
            "Hi.\tGrüß Gott!\n",
            "Run!\tLauf!\n",
            "Wow!\tPotzdonner!\n",
            "Wow!\tDonnerwetter!\n",
            "Fire!\tFeuer!\n",
            "Help!\tHilfe!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "text=load_file(data)\n",
        "print(text[:99])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "sXU5Hcv8LTYh"
      },
      "outputs": [],
      "source": [
        "#split the text by phrases\n",
        "def to_phrase(doc):\n",
        "  lines = doc.strip().split('\\n')\n",
        "  phrases =[line.split('\\t') for line in lines]\n",
        "  return phrases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXWjh7BjO01Z",
        "outputId": "f69802e1-5320-4255-9e99-7f15e62e5328"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['Hi.', 'Hallo!'], ['Hi.', 'Grüß Gott!'], ['Run!', 'Lauf!']]\n",
            "<class 'list'>\n"
          ]
        }
      ],
      "source": [
        "phrases=to_phrase(text)\n",
        "print(phrases[:3])\n",
        "print(type(phrases))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ierstHGpZkP_",
        "outputId": "3b6485aa-c6c4-466a-8a91-148f62601f64"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~ \\t\\n\\r\\x0b\\x0c'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "string.printable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "1vNWE1c4O9ZE"
      },
      "outputs": [],
      "source": [
        "def clean(lines):\n",
        "    cleaned = []\n",
        "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
        "    table = str.maketrans('','', string.punctuation)\n",
        "\n",
        "    for l in lines:\n",
        "        clean_pair=[]\n",
        "        for x in l :\n",
        "            x = normalize('NFD',x).encode('ascii','ignore')\n",
        "            x = x.decode('UTF-8')\n",
        "            x = x.split()\n",
        "            x = [word.lower() for word in x]\n",
        "            x = [word.translate(table) for word in x]\n",
        "            x = [re_print.sub('' , word) for word in x]\n",
        "            x = [word for word in x if word.isalpha()]\n",
        "            clean_pair.append(' '.join(x))\n",
        "        cleaned.append(clean_pair)\n",
        "    return array(cleaned)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "J8KP7FMSaumf"
      },
      "outputs": [],
      "source": [
        "# save a list of clean sentences to file\n",
        "def save_clean_data(sentences, filename):\n",
        "  dump(sentences, open(filename, 'wb'))\n",
        "  print('Saved: %s' % filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "L0P69skBdES1"
      },
      "outputs": [],
      "source": [
        "doc = load_file(filename=data)\n",
        "pairs = to_phrase(doc)\n",
        "clean_pairs = clean(pairs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clean_pairs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfh-pq461RDR",
        "outputId": "c16f9b22-74a5-448a-b5a7-70fd14b218b3"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([['hi', 'hallo'],\n",
              "       ['hi', 'gru gott'],\n",
              "       ['run', 'lauf'],\n",
              "       ...,\n",
              "       ['if someone who doesnt know your background says that you sound like a native speaker it means they probably noticed something about your speaking that made them realize you werent a native speaker in other words you dont really sound like a native speaker',\n",
              "        'wenn jemand der deine herkunft nicht kennt sagt dass du wie ein muttersprachler sprichst bedeutet das dass man wahrscheinlich etwas an deiner sprechweise bemerkt hat das erkennen lie dass du kein muttersprachler bist mit anderen worten du horst dich nicht wirklich wie ein muttersprachler an'],\n",
              "       ['if someone who doesnt know your background says that you sound like a native speaker it means they probably noticed something about your speaking that made them realize you werent a native speaker in other words you dont really sound like a native speaker',\n",
              "        'wenn jemand fremdes dir sagt dass du dich wie ein muttersprachler anhorst bedeutet das wahrscheinlich er hat etwas an deinem sprechen bemerkt dass dich als nichtmuttersprachler verraten hat mit anderen worten du horst dich nicht wirklich wie ein muttersprachler an'],\n",
              "       ['it may be impossible to get a completely errorfree corpus due to the nature of this kind of collaborative effort however if we encourage members to contribute sentences in their own languages rather than experiment in languages they are learning we might be able to minimize errors',\n",
              "        'es ist wohl unmoglich einen vollkommen fehlerfreien korpus zu erreichen das liegt in der natur eines solchen gemeinschaftsprojekts doch wenn wir unsere mitglieder dazu bringen konnen nicht mit sprachen herumzuexperimentieren die sie gerade lernen sondern satze in ihrer eigenen muttersprache beizutragen dann gelingt es uns vielleicht die zahl der fehler klein zu halten']],\n",
              "      dtype='<U370')"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXmw6s_adgFy",
        "outputId": "a58ebf8a-6d92-4622-e009-34485cbb6e5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: ENGLISH-GERMAN.pkl\n"
          ]
        }
      ],
      "source": [
        "save_clean_data(clean_pairs, 'ENGLISH-GERMAN.pkl') # Run Only Once."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "uo5iKjB2dp7k"
      },
      "outputs": [],
      "source": [
        "#train-test-split\n",
        "\n",
        "def load_clean_sentences(filename):\n",
        " return load(open(filename, 'rb'))\n",
        "\n",
        "raw_dataset = load_clean_sentences('ENGLISH-GERMAN.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBOsy9lskcJ6",
        "outputId": "699f534a-9c88-48ed-c166-6585b2c2f36c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: EG-BOTH.pkl\n",
            "Saved: EG-TRAIN.pkl\n",
            "Saved: EG-TEST.pkl\n"
          ]
        }
      ],
      "source": [
        "n_sentences = 10000\n",
        "dataset = raw_dataset[:n_sentences, :]\n",
        "shuffle(dataset)\n",
        "train, test = dataset[:9000], dataset[9000:]\n",
        "save_clean_data(dataset, 'EG-BOTH.pkl')\n",
        "save_clean_data(train, 'EG-TRAIN.pkl')\n",
        "save_clean_data(test, 'EG-TEST.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "XMukHFygklm8"
      },
      "outputs": [],
      "source": [
        "#Train Neural Translation model\n",
        "#Start by loading the train - test data stored in pkl files\n",
        "# Step 7 : Load the pkl files\n",
        "\n",
        "dataset = load_clean_sentences('EG-BOTH.pkl')\n",
        "train = load_clean_sentences('EG-TRAIN.pkl')\n",
        "test = load_clean_sentences('EG-TEST.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "QYPnaNA3qJV-"
      },
      "outputs": [],
      "source": [
        "#We can use the Keras Tokenize class to map words to integers, as needed for modeling.\n",
        "#We will use separate tokenizer for the English sequences and the German sequences.\n",
        "#The function below-named create_tokenizer() will train a tokenizer on a list of phrases\n",
        "#Step 8 : Create tokenizer\n",
        "def create_tokenizer(lines):\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(lines)\n",
        "    return tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "Qk66q7zPqpIr"
      },
      "outputs": [],
      "source": [
        "# Find the length of longest sequence in the list of phrases\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        " return max(len(line.split()) for line in lines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zeCGLEOnq4vX",
        "outputId": "b5e81afe-8d6d-48e8-ca48-26704420d9a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English Vocabulary Size: 2404\n",
            "English Max Length: 5\n"
          ]
        }
      ],
      "source": [
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
        "print('English Max Length: %d' % (eng_length))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EfAzvLrNrSmb",
        "outputId": "371090a6-de30-42f9-f094-686879671ed9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "German Vocabulary Size: 3856\n",
            "German Max Length: 5\n"
          ]
        }
      ],
      "source": [
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 0])\n",
        "print('German Vocabulary Size: %d' % ger_vocab_size)\n",
        "print('German Max Length: %d' % (ger_length))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bnkzCgQarZjV"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}