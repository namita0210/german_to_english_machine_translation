{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPHoronm3xaWv/hkPsAhWMS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/namita0210/german_to_english_machine_translation/blob/main/German_to_English_Machine_Translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "from numpy import array\n",
        "from pickle import dump\n",
        "from unicodedata import normalize\n",
        "from pickle import load\n",
        "from pickle import dump\n",
        "from numpy.random import rand\n",
        "from numpy.random import shuffle\n",
        "\n",
        "from pickle import load\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "#from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.callbacks import ModelCheckpoint"
      ],
      "metadata": {
        "id": "NHdFZ05SX0RQ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SpUqCSOiJq19",
        "outputId": "398c17af-9f61-480d-d670-5acdf0964e23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "data = '/content/drive/MyDrive/deu.txt'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#function to load the file and preserve the unicode german characters\n",
        "def load_file(filename):\n",
        "  file = open(filename , 'r', encoding='utf-8')\n",
        "  text = file.read()\n",
        "  file.close()\n",
        "  return text"
      ],
      "metadata": {
        "id": "zs2n5AAzKyd5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text=load_file(data)\n",
        "print(text[:99])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dit3yCEmLQB3",
        "outputId": "befbfee0-92ba-422a-991f-2b45874b43a3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hi.\tHallo!\n",
            "Hi.\tGrüß Gott!\n",
            "Run!\tLauf!\n",
            "Wow!\tPotzdonner!\n",
            "Wow!\tDonnerwetter!\n",
            "Fire!\tFeuer!\n",
            "Help!\tHilfe!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#split the text by phrases\n",
        "def to_phrase(doc):\n",
        "  lines = doc.strip().split('\\n')\n",
        "  phrases =[line.split('\\t') for line in lines]\n",
        "  return phrases"
      ],
      "metadata": {
        "id": "sXU5Hcv8LTYh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "phrases=to_phrase(text)\n",
        "print(phrases[:3])\n",
        "print(type(phrases))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXWjh7BjO01Z",
        "outputId": "b0dacc96-6bfb-4a30-bf17-f3bcbbfd7e52"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['Hi.', 'Hallo!'], ['Hi.', 'Grüß Gott!'], ['Run!', 'Lauf!']]\n",
            "<class 'list'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "string.printable"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ierstHGpZkP_",
        "outputId": "09373998-c642-402d-c6cb-dbf68e6723ed"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~ \\t\\n\\r\\x0b\\x0c'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#clean a list of lines\n",
        "def clean_phrase(lines):\n",
        "  cleaned = []\n",
        "  re_print = re.compile('[^%s]' % re.escape(string.printable)) #remove non-printable characters\n",
        "  table = str.maketrans('','',string.punctuation)\n",
        "  for pair in lines:\n",
        "    clean_pair=[]\n",
        "    for p in pair:\n",
        "      p = normalize('NFD',p).encode('ascii','ignore')\n",
        "      p = p.decode('UTF-8')\n",
        "      p = p.split()\n",
        "      p = [word.lower() for word in p]\n",
        "      p = [word.translate(table) for word in p] # remove punctuation\n",
        "      p = [re_print.sub('',w) for w in p]\n",
        "      p = [word for word in p if word.isalpha()]\n",
        "    clean_pair.append(''.join(p))\n",
        "  cleaned.append(clean_pair)\n",
        "  return array(cleaned)"
      ],
      "metadata": {
        "id": "1vNWE1c4O9ZE"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save a list of clean sentences to file\n",
        "def save_clean_data(sentences, filename):\n",
        "  dump(sentences, open(filename, 'wb'))\n",
        "  print('Saved: %s' % filename)"
      ],
      "metadata": {
        "id": "J8KP7FMSaumf"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = load_file(filename=data)\n",
        "pairs = to_phrase(doc)\n",
        "clean_pairs = clean_phrase(pairs)"
      ],
      "metadata": {
        "id": "L0P69skBdES1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_clean_data(clean_pairs, 'english-german.pkl') # Run Only Once."
      ],
      "metadata": {
        "id": "WXmw6s_adgFy",
        "outputId": "e53bc2b2-b43a-48f3-c6a3-ea3ba6c0a19c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: english-german.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#train-test-split\n",
        "\n",
        "def load_clean_sentences(filename):\n",
        " return load(open(filename, 'rb'))\n",
        "\n",
        "raw_dataset = load_clean_sentences('english-german.pkl')"
      ],
      "metadata": {
        "id": "uo5iKjB2dp7k"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_sentences = 10000\n",
        "dataset = raw_dataset[:n_sentences, :]\n",
        "shuffle(dataset)\n",
        "train, test = dataset[:9000], dataset[9000:]\n",
        "save_clean_data(dataset, 'english-german-both.pkl')\n",
        "save_clean_data(train, 'english-german-train.pkl')\n",
        "save_clean_data(test, 'english-german-test.pkl')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBOsy9lskcJ6",
        "outputId": "09340d54-6327-4ee0-bfe8-da95f1251cd6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: english-german-both.pkl\n",
            "Saved: english-german-train.pkl\n",
            "Saved: english-german-test.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Train Neural Translation model\n",
        "#Start by loading the train - test data stored in pkl files\n",
        "# Step 7 : Load the pkl files\n",
        "\n",
        "dataset = load_clean_sentences('english-german-both.pkl')\n",
        "train = load_clean_sentences('english-german-train.pkl')\n",
        "test = load_clean_sentences('english-german-test.pkl')"
      ],
      "metadata": {
        "id": "XMukHFygklm8"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We can use the Keras Tokenize class to map words to integers, as needed for modeling.\n",
        "#We will use separate tokenizer for the English sequences and the German sequences.\n",
        "#The function below-named create_tokenizer() will train a tokenizer on a list of phrases\n",
        "#Step 8 : Create tokenizer\n",
        "def create_tokenizer(lines):\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(lines)\n",
        "    return tokenizer"
      ],
      "metadata": {
        "id": "QYPnaNA3qJV-"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the length of longest sequence in the list of phrases\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        " return max(len(line.split()) for line in lines)"
      ],
      "metadata": {
        "id": "Qk66q7zPqpIr"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zeCGLEOnq4vX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}