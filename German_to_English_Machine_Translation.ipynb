{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/namita0210/german_to_english_machine_translation/blob/main/German_to_English_Machine_Translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NHdFZ05SX0RQ"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "from numpy import array\n",
        "from pickle import dump\n",
        "from unicodedata import normalize\n",
        "from pickle import load\n",
        "from pickle import dump\n",
        "from numpy.random import rand\n",
        "from numpy.random import shuffle\n",
        "from pickle import load\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from numpy import argmax\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SpUqCSOiJq19",
        "outputId": "f95d5450-87c2-49e4-f776-9f730a26d77c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "data = '/content/drive/MyDrive/deu.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zs2n5AAzKyd5"
      },
      "outputs": [],
      "source": [
        "#function to load the file and preserve the unicode german characters\n",
        "def load_file(filename):\n",
        "  file = open(filename , 'r', encoding='utf-8')\n",
        "  text = file.read()\n",
        "  file.close()\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dit3yCEmLQB3",
        "outputId": "eddbd5d6-f164-4097-8633-46fb37e5b137"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hi.\tHallo!\n",
            "Hi.\tGrüß Gott!\n",
            "Run!\tLauf!\n",
            "Wow!\tPotzdonner!\n",
            "Wow!\tDonnerwetter!\n",
            "Fire!\tFeuer!\n",
            "Help!\tHilfe!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "text=load_file(data)\n",
        "print(text[:99])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXU5Hcv8LTYh"
      },
      "outputs": [],
      "source": [
        "#split the text by phrases\n",
        "def to_phrase(doc):\n",
        "  lines = doc.strip().split('\\n')\n",
        "  phrases =[line.split('\\t') for line in lines]\n",
        "  return phrases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXWjh7BjO01Z",
        "outputId": "f69802e1-5320-4255-9e99-7f15e62e5328"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['Hi.', 'Hallo!'], ['Hi.', 'Grüß Gott!'], ['Run!', 'Lauf!']]\n",
            "<class 'list'>\n"
          ]
        }
      ],
      "source": [
        "phrases=to_phrase(text)\n",
        "print(phrases[:3])\n",
        "print(type(phrases))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ierstHGpZkP_",
        "outputId": "3b6485aa-c6c4-466a-8a91-148f62601f64"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~ \\t\\n\\r\\x0b\\x0c'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "string.printable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1vNWE1c4O9ZE"
      },
      "outputs": [],
      "source": [
        "def clean(lines):\n",
        "    cleaned = []\n",
        "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
        "    table = str.maketrans('','', string.punctuation)\n",
        "\n",
        "    for l in lines:\n",
        "        clean_pair=[]\n",
        "        for x in l :\n",
        "            x = normalize('NFD',x).encode('ascii','ignore')\n",
        "            x = x.decode('UTF-8')\n",
        "            x = x.split()\n",
        "            x = [word.lower() for word in x]\n",
        "            x = [word.translate(table) for word in x]\n",
        "            x = [re_print.sub('' , word) for word in x]\n",
        "            x = [word for word in x if word.isalpha()]\n",
        "            clean_pair.append(' '.join(x))\n",
        "        cleaned.append(clean_pair)\n",
        "    return array(cleaned)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J8KP7FMSaumf"
      },
      "outputs": [],
      "source": [
        "# save a list of clean sentences to file\n",
        "def save_clean_data(sentences, filename):\n",
        "  dump(sentences, open(filename, 'wb'))\n",
        "  print('Saved: %s' % filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0P69skBdES1"
      },
      "outputs": [],
      "source": [
        "doc = load_file(filename=data)\n",
        "pairs = to_phrase(doc)\n",
        "clean_pairs = clean(pairs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clean_pairs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfh-pq461RDR",
        "outputId": "c16f9b22-74a5-448a-b5a7-70fd14b218b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([['hi', 'hallo'],\n",
              "       ['hi', 'gru gott'],\n",
              "       ['run', 'lauf'],\n",
              "       ...,\n",
              "       ['if someone who doesnt know your background says that you sound like a native speaker it means they probably noticed something about your speaking that made them realize you werent a native speaker in other words you dont really sound like a native speaker',\n",
              "        'wenn jemand der deine herkunft nicht kennt sagt dass du wie ein muttersprachler sprichst bedeutet das dass man wahrscheinlich etwas an deiner sprechweise bemerkt hat das erkennen lie dass du kein muttersprachler bist mit anderen worten du horst dich nicht wirklich wie ein muttersprachler an'],\n",
              "       ['if someone who doesnt know your background says that you sound like a native speaker it means they probably noticed something about your speaking that made them realize you werent a native speaker in other words you dont really sound like a native speaker',\n",
              "        'wenn jemand fremdes dir sagt dass du dich wie ein muttersprachler anhorst bedeutet das wahrscheinlich er hat etwas an deinem sprechen bemerkt dass dich als nichtmuttersprachler verraten hat mit anderen worten du horst dich nicht wirklich wie ein muttersprachler an'],\n",
              "       ['it may be impossible to get a completely errorfree corpus due to the nature of this kind of collaborative effort however if we encourage members to contribute sentences in their own languages rather than experiment in languages they are learning we might be able to minimize errors',\n",
              "        'es ist wohl unmoglich einen vollkommen fehlerfreien korpus zu erreichen das liegt in der natur eines solchen gemeinschaftsprojekts doch wenn wir unsere mitglieder dazu bringen konnen nicht mit sprachen herumzuexperimentieren die sie gerade lernen sondern satze in ihrer eigenen muttersprache beizutragen dann gelingt es uns vielleicht die zahl der fehler klein zu halten']],\n",
              "      dtype='<U370')"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXmw6s_adgFy",
        "outputId": "a58ebf8a-6d92-4622-e009-34485cbb6e5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: ENGLISH-GERMAN.pkl\n"
          ]
        }
      ],
      "source": [
        "save_clean_data(clean_pairs, 'ENGLISH-GERMAN.pkl') # Run Only Once."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uo5iKjB2dp7k"
      },
      "outputs": [],
      "source": [
        "#train-test-split\n",
        "\n",
        "def load_clean_sentences(filename):\n",
        " return load(open(filename, 'rb'))\n",
        "\n",
        "raw_dataset = load_clean_sentences('ENGLISH-GERMAN.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBOsy9lskcJ6",
        "outputId": "699f534a-9c88-48ed-c166-6585b2c2f36c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: EG-BOTH.pkl\n",
            "Saved: EG-TRAIN.pkl\n",
            "Saved: EG-TEST.pkl\n"
          ]
        }
      ],
      "source": [
        "n_sentences = 10000\n",
        "dataset = raw_dataset[:n_sentences, :]\n",
        "shuffle(dataset)\n",
        "train, test = dataset[:9000], dataset[9000:]\n",
        "save_clean_data(dataset, 'EG-BOTH.pkl')\n",
        "save_clean_data(train, 'EG-TRAIN.pkl')\n",
        "save_clean_data(test, 'EG-TEST.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMukHFygklm8"
      },
      "outputs": [],
      "source": [
        "#Train Neural Translation model\n",
        "#Start by loading the train - test data stored in pkl files\n",
        "# Step 7 : Load the pkl files\n",
        "\n",
        "dataset = load_clean_sentences('EG-BOTH.pkl')\n",
        "train = load_clean_sentences('EG-TRAIN.pkl')\n",
        "test = load_clean_sentences('EG-TEST.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYPnaNA3qJV-"
      },
      "outputs": [],
      "source": [
        "#We can use the Keras Tokenize class to map words to integers, as needed for modeling.\n",
        "#We will use separate tokenizer for the English sequences and the German sequences.\n",
        "#The function below-named create_tokenizer() will train a tokenizer on a list of phrases\n",
        "#Step 8 : Create tokenizer\n",
        "def create_tokenizer(lines):\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(lines)\n",
        "    return tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qk66q7zPqpIr"
      },
      "outputs": [],
      "source": [
        "# Find the length of longest sequence in the list of phrases\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        " return max(len(line.split()) for line in lines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zeCGLEOnq4vX",
        "outputId": "b5e81afe-8d6d-48e8-ca48-26704420d9a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English Vocabulary Size: 2404\n",
            "English Max Length: 5\n"
          ]
        }
      ],
      "source": [
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
        "print('English Max Length: %d' % (eng_length))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EfAzvLrNrSmb",
        "outputId": "371090a6-de30-42f9-f094-686879671ed9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "German Vocabulary Size: 3856\n",
            "German Max Length: 5\n"
          ]
        }
      ],
      "source": [
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 0])\n",
        "print('German Vocabulary Size: %d' % ger_vocab_size)\n",
        "print('German Max Length: %d' % (ger_length))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bnkzCgQarZjV"
      },
      "outputs": [],
      "source": [
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        " X = tokenizer.texts_to_sequences(lines)\n",
        " X = pad_sequences(X, maxlen=length, padding='post')\n",
        " return X"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# one hot encode target sequence\n",
        "def encode_output(sequences, vocab_size):\n",
        "\tylist = list()\n",
        "\tfor sequence in sequences:\n",
        "\t\tencoded = to_categorical(sequence, num_classes=vocab_size)\n",
        "\t\tylist.append(encoded)\n",
        "\ty = array(ylist)\n",
        "\ty = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
        "\treturn y"
      ],
      "metadata": {
        "id": "QleuHz9t5FuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare training data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
        "trainY = encode_output(trainY, eng_vocab_size)\n",
        "# prepare validation data\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
        "testY = encode_output(testY, eng_vocab_size)"
      ],
      "metadata": {
        "id": "O2Obz-e85UDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define NMT model\n",
        "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
        "\tmodel.add(LSTM(n_units))\n",
        "\tmodel.add(RepeatVector(tar_timesteps))\n",
        "\tmodel.add(LSTM(n_units, return_sequences=True))\n",
        "\tmodel.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
        "\treturn model"
      ],
      "metadata": {
        "id": "aFYqxzko5YPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define model\n",
        "model = define_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 256)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')"
      ],
      "metadata": {
        "id": "d-5__pJI5tDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# summarize defined model\n",
        "print(model.summary())"
      ],
      "metadata": {
        "id": "8CLSoxwr52Uz",
        "outputId": "d2cd4b6d-d6f5-4388-ca57-44822097d3a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 5, 256)            987136    \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 256)               525312    \n",
            "                                                                 \n",
            " repeat_vector_1 (RepeatVec  (None, 5, 256)            0         \n",
            " tor)                                                            \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               (None, 5, 256)            525312    \n",
            "                                                                 \n",
            " time_distributed_1 (TimeDi  (None, 5, 2404)           617828    \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2655588 (10.13 MB)\n",
            "Trainable params: 2655588 (10.13 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# fit model\n",
        "filename = 'model.h5'\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "model.fit(trainX, trainY, epochs=30, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)"
      ],
      "metadata": {
        "id": "sw7RcUj655Np",
        "outputId": "ce9b0b4e-b7a6-4b5d-c154-2c140c06d1fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\n",
            "Epoch 1: val_loss improved from inf to 3.68061, saving model to model.h5\n",
            "141/141 - 33s - loss: 4.4289 - val_loss: 3.6806 - 33s/epoch - 233ms/step\n",
            "Epoch 2/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2: val_loss improved from 3.68061 to 3.55321, saving model to model.h5\n",
            "141/141 - 21s - loss: 3.5508 - val_loss: 3.5532 - 21s/epoch - 149ms/step\n",
            "Epoch 3/30\n",
            "\n",
            "Epoch 3: val_loss improved from 3.55321 to 3.45684, saving model to model.h5\n",
            "141/141 - 20s - loss: 3.4023 - val_loss: 3.4568 - 20s/epoch - 138ms/step\n",
            "Epoch 4/30\n",
            "\n",
            "Epoch 4: val_loss improved from 3.45684 to 3.36912, saving model to model.h5\n",
            "141/141 - 20s - loss: 3.2528 - val_loss: 3.3691 - 20s/epoch - 140ms/step\n",
            "Epoch 5/30\n",
            "\n",
            "Epoch 5: val_loss improved from 3.36912 to 3.27230, saving model to model.h5\n",
            "141/141 - 21s - loss: 3.1246 - val_loss: 3.2723 - 21s/epoch - 151ms/step\n",
            "Epoch 6/30\n",
            "\n",
            "Epoch 6: val_loss improved from 3.27230 to 3.19573, saving model to model.h5\n",
            "141/141 - 21s - loss: 3.0073 - val_loss: 3.1957 - 21s/epoch - 147ms/step\n",
            "Epoch 7/30\n",
            "\n",
            "Epoch 7: val_loss improved from 3.19573 to 3.09409, saving model to model.h5\n",
            "141/141 - 28s - loss: 2.8749 - val_loss: 3.0941 - 28s/epoch - 197ms/step\n",
            "Epoch 8/30\n",
            "\n",
            "Epoch 8: val_loss improved from 3.09409 to 3.02535, saving model to model.h5\n",
            "141/141 - 29s - loss: 2.7482 - val_loss: 3.0254 - 29s/epoch - 205ms/step\n",
            "Epoch 9/30\n",
            "\n",
            "Epoch 9: val_loss improved from 3.02535 to 2.93115, saving model to model.h5\n",
            "141/141 - 22s - loss: 2.6169 - val_loss: 2.9311 - 22s/epoch - 157ms/step\n",
            "Epoch 10/30\n",
            "\n",
            "Epoch 10: val_loss improved from 2.93115 to 2.84772, saving model to model.h5\n",
            "141/141 - 21s - loss: 2.4811 - val_loss: 2.8477 - 21s/epoch - 146ms/step\n",
            "Epoch 11/30\n",
            "\n",
            "Epoch 11: val_loss improved from 2.84772 to 2.76622, saving model to model.h5\n",
            "141/141 - 20s - loss: 2.3375 - val_loss: 2.7662 - 20s/epoch - 140ms/step\n",
            "Epoch 12/30\n",
            "\n",
            "Epoch 12: val_loss improved from 2.76622 to 2.67406, saving model to model.h5\n",
            "141/141 - 22s - loss: 2.1967 - val_loss: 2.6741 - 22s/epoch - 155ms/step\n",
            "Epoch 13/30\n",
            "\n",
            "Epoch 13: val_loss improved from 2.67406 to 2.62000, saving model to model.h5\n",
            "141/141 - 21s - loss: 2.0538 - val_loss: 2.6200 - 21s/epoch - 151ms/step\n",
            "Epoch 14/30\n",
            "\n",
            "Epoch 14: val_loss improved from 2.62000 to 2.55011, saving model to model.h5\n",
            "141/141 - 22s - loss: 1.9289 - val_loss: 2.5501 - 22s/epoch - 154ms/step\n",
            "Epoch 15/30\n",
            "\n",
            "Epoch 15: val_loss improved from 2.55011 to 2.49091, saving model to model.h5\n",
            "141/141 - 25s - loss: 1.8020 - val_loss: 2.4909 - 25s/epoch - 180ms/step\n",
            "Epoch 16/30\n",
            "\n",
            "Epoch 16: val_loss improved from 2.49091 to 2.45373, saving model to model.h5\n",
            "141/141 - 21s - loss: 1.6882 - val_loss: 2.4537 - 21s/epoch - 148ms/step\n",
            "Epoch 17/30\n",
            "\n",
            "Epoch 17: val_loss improved from 2.45373 to 2.39171, saving model to model.h5\n",
            "141/141 - 22s - loss: 1.5803 - val_loss: 2.3917 - 22s/epoch - 159ms/step\n",
            "Epoch 18/30\n",
            "\n",
            "Epoch 18: val_loss improved from 2.39171 to 2.36699, saving model to model.h5\n",
            "141/141 - 20s - loss: 1.4769 - val_loss: 2.3670 - 20s/epoch - 143ms/step\n",
            "Epoch 19/30\n",
            "\n",
            "Epoch 19: val_loss improved from 2.36699 to 2.31824, saving model to model.h5\n",
            "141/141 - 23s - loss: 1.3774 - val_loss: 2.3182 - 23s/epoch - 167ms/step\n",
            "Epoch 20/30\n",
            "\n",
            "Epoch 20: val_loss improved from 2.31824 to 2.29301, saving model to model.h5\n",
            "141/141 - 20s - loss: 1.2859 - val_loss: 2.2930 - 20s/epoch - 142ms/step\n",
            "Epoch 21/30\n",
            "\n",
            "Epoch 21: val_loss improved from 2.29301 to 2.26781, saving model to model.h5\n",
            "141/141 - 20s - loss: 1.1954 - val_loss: 2.2678 - 20s/epoch - 139ms/step\n",
            "Epoch 22/30\n",
            "\n",
            "Epoch 22: val_loss improved from 2.26781 to 2.23989, saving model to model.h5\n",
            "141/141 - 23s - loss: 1.1124 - val_loss: 2.2399 - 23s/epoch - 160ms/step\n",
            "Epoch 23/30\n",
            "\n",
            "Epoch 23: val_loss improved from 2.23989 to 2.22525, saving model to model.h5\n",
            "141/141 - 20s - loss: 1.0315 - val_loss: 2.2252 - 20s/epoch - 139ms/step\n",
            "Epoch 24/30\n",
            "\n",
            "Epoch 24: val_loss improved from 2.22525 to 2.19744, saving model to model.h5\n",
            "141/141 - 20s - loss: 0.9548 - val_loss: 2.1974 - 20s/epoch - 140ms/step\n",
            "Epoch 25/30\n",
            "\n",
            "Epoch 25: val_loss improved from 2.19744 to 2.18792, saving model to model.h5\n",
            "141/141 - 21s - loss: 0.8800 - val_loss: 2.1879 - 21s/epoch - 146ms/step\n",
            "Epoch 26/30\n",
            "\n",
            "Epoch 26: val_loss improved from 2.18792 to 2.17864, saving model to model.h5\n",
            "141/141 - 21s - loss: 0.8100 - val_loss: 2.1786 - 21s/epoch - 148ms/step\n",
            "Epoch 27/30\n",
            "\n",
            "Epoch 27: val_loss improved from 2.17864 to 2.16967, saving model to model.h5\n",
            "141/141 - 20s - loss: 0.7463 - val_loss: 2.1697 - 20s/epoch - 139ms/step\n",
            "Epoch 28/30\n",
            "\n",
            "Epoch 28: val_loss improved from 2.16967 to 2.16234, saving model to model.h5\n",
            "141/141 - 20s - loss: 0.6889 - val_loss: 2.1623 - 20s/epoch - 139ms/step\n",
            "Epoch 29/30\n",
            "\n",
            "Epoch 29: val_loss improved from 2.16234 to 2.15771, saving model to model.h5\n",
            "141/141 - 21s - loss: 0.6313 - val_loss: 2.1577 - 21s/epoch - 150ms/step\n",
            "Epoch 30/30\n",
            "\n",
            "Epoch 30: val_loss improved from 2.15771 to 2.15080, saving model to model.h5\n",
            "141/141 - 20s - loss: 0.5784 - val_loss: 2.1508 - 20s/epoch - 144ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7be89d267880>"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate"
      ],
      "metadata": {
        "id": "iXHYxwVZ6fNj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load datasets\n",
        "dataset = load_clean_sentences('EG-BOTH.pkl')\n",
        "train = load_clean_sentences('EG-TRAIN.pkl')\n",
        "test = load_clean_sentences('EG-TEST.pkl')\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "# prepare data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])"
      ],
      "metadata": {
        "id": "0CQ4oFEu59JL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load model\n",
        "model = load_model('model.h5')"
      ],
      "metadata": {
        "id": "XFaNoHOn6vZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate = model.predict()"
      ],
      "metadata": {
        "id": "0fFgZTDiUeQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None"
      ],
      "metadata": {
        "id": "0hNrhQp-7OnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate target given source sequence\n",
        "def predict_sequence(model, tokenizer, source):\n",
        "\tprediction = model.predict(source, verbose=0)[0]\n",
        "\tintegers = [argmax(vector) for vector in prediction]\n",
        "\ttarget = list()\n",
        "\tfor i in integers:\n",
        "\t\tword = word_for_id(i, tokenizer)\n",
        "\t\tif word is None:\n",
        "\t\t\tbreak\n",
        "\t\ttarget.append(word)\n",
        "\treturn ' '.join(target)"
      ],
      "metadata": {
        "id": "8Sn9ElsE7Vey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate the skill of the model\n",
        "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
        "\tactual, predicted = list(), list()\n",
        "\tfor i, source in enumerate(sources):\n",
        "\t\t# translate encoded source text\n",
        "\t\tsource = source.reshape((1, source.shape[0]))\n",
        "\t\ttranslation = predict_sequence(model, eng_tokenizer, source)\n",
        "\t\traw_target, raw_src = raw_dataset[i]\n",
        "\t\tif i < 10:\n",
        "\t\t\tprint('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
        "\t\tactual.append([raw_target.split()])\n",
        "\t\tpredicted.append(translation.split())\n",
        "\t# calculate BLEU score\n",
        "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"
      ],
      "metadata": {
        "id": "GTRZjpeN7aag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R8TmZtLL9E0m"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}